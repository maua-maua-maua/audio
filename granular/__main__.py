from typing import List

import fire

from granular.inference import generate
from granular.train_stage1 import stage1
from granular.train_stage2 import stage2
from granular.train_stage3 import stage3


class Pipeline(object):
    def train_all(
        self,
        data_dir: str,
        name=None,
        classes: List[str] = [],
        gpus=1,
        profiler=False,
        out_dir="modelzoo",
        # waveform training config
        waveform_name=None,
        waveform_continue_train=False,
        waveform_batch_size=24,
        waveform_learning_rate=0.0002,
        waveform_max_steps=300000,
        waveform_num_workers=2,
        waveform_precision=32,
        waveform_tar_beta=0.003,
        waveform_beta_steps=500,
        waveform_tar_l=1.1,
        # waveform model config
        waveform_amplitude_norm=False,
        waveform_channels=128,
        waveform_env_dist=0,
        waveform_h_dim=512,
        waveform_kernel_size=9,
        waveform_l_grain=2048,
        waveform_log_dist=0.0,
        waveform_mel_dist=True,
        waveform_mel_scales=[2048, 1024],
        waveform_n_convs=3,
        waveform_n_linears=3,
        waveform_normalize_ola=True,
        waveform_pp_chans=5,
        waveform_pp_ker=65,
        waveform_silent_reject=[0.2, 0.2],
        waveform_spec_power=1,
        waveform_sr=22050,
        waveform_stft_scales=[2048, 1024, 512, 256],
        waveform_stride=4,
        waveform_z_dim=128,
        # latent training config
        latent_name=None,
        latent_batch_size=32,
        latent_learning_rate=0.0002,
        latent_max_steps=500000,
        latent_tar_beta=0.01,
        latent_beta_steps=500,
        latent_num_workers=2,
        latent_precision=32,
        # latent model config
        latent_e_dim=256,
        latent_h_dim=512,
        latent_n_RNN=1,
        latent_n_linears=2,
        latent_rnn_type="LSTM",
        # hierarchical training config
        hierarchical_batch_size=16,
        hierarchical_learning_rate=2e-6,  # here is the fixed learning rate at the end of the decay of the sub-network pretraining
        hierarchical_w_beta=0.0,
        hierarchical_l_beta=0.0,
        hierarchical_max_steps=100000,
        hierarchical_num_workers=2,
        hierarchical_precision=32,
    ):
        self.train_waveform(
            data_dir=data_dir,
            classes=classes,
            name=waveform_name if waveform_name is not None else f"granular_waveform_{name}",
            continue_train=waveform_continue_train,
            batch_size=waveform_batch_size,
            learning_rate=waveform_learning_rate,
            max_steps=waveform_max_steps,
            num_workers=waveform_num_workers,
            gpus=gpus,
            precision=waveform_precision,
            profiler=profiler,
            out_dir=out_dir,
            tar_beta=waveform_tar_beta,
            beta_steps=waveform_beta_steps,
            tar_l=waveform_tar_l,
            amplitude_norm=waveform_amplitude_norm,
            channels=waveform_channels,
            env_dist=waveform_env_dist,
            h_dim=waveform_h_dim,
            kernel_size=waveform_kernel_size,
            l_grain=waveform_l_grain,
            log_dist=waveform_log_dist,
            mel_dist=waveform_mel_dist,
            mel_scales=waveform_mel_scales,
            n_convs=waveform_n_convs,
            n_linears=waveform_n_linears,
            normalize_ola=waveform_normalize_ola,
            pp_chans=waveform_pp_chans,
            pp_ker=waveform_pp_ker,
            silent_reject=waveform_silent_reject,
            spec_power=waveform_spec_power,
            sr=waveform_sr,
            stft_scales=waveform_stft_scales,
            stride=waveform_stride,
            z_dim=waveform_z_dim,
        )
        self.train_latent(
            data_dir=data_dir,
            name=latent_name if latent_name is not None else f"granular_latent_{name}",
            waveform_name=waveform_name if waveform_name is not None else f"granular_waveform_{name}",
            batch_size=latent_batch_size,
            learning_rate=latent_learning_rate,
            max_steps=latent_max_steps,
            tar_beta=latent_tar_beta,
            beta_steps=latent_beta_steps,
            conditional=len(classes) != 0,
            num_workers=latent_num_workers,
            gpus=gpus,
            precision=latent_precision,
            profiler=profiler,
            out_dir=out_dir,
            e_dim=latent_e_dim,
            h_dim=latent_h_dim,
            n_RNN=latent_n_RNN,
            n_linears=latent_n_linears,
            rnn_type=latent_rnn_type,
        )
        self.train_hierarchical(
            data_dir=data_dir,
            latent_name=latent_name if latent_name is not None else f"granular_latent_{name}",
            waveform_name=waveform_name if waveform_name is not None else f"granular_waveform_{name}",
            batch_size=hierarchical_batch_size,
            learning_rate=hierarchical_learning_rate,
            w_beta=hierarchical_w_beta,
            l_beta=hierarchical_l_beta,
            max_steps=hierarchical_max_steps,
            num_workers=hierarchical_num_workers,
            gpus=gpus,
            precision=hierarchical_precision,
            profiler=profiler,
            out_dir=out_dir,
        )

    def train_waveform(
        self,
        data_dir: str,
        classes=[],
        name=None,
        continue_train=False,
        batch_size=24,
        learning_rate=0.0002,
        max_steps=300000,
        num_workers=2,
        gpus=1,
        precision=32,
        profiler=False,
        out_dir="modelzoo",
        tar_beta=0.003,
        beta_steps=500,
        tar_l=1.1,
        # waveform model config
        amplitude_norm=False,
        channels=128,
        env_dist=0,
        h_dim=512,
        kernel_size=9,
        l_grain=2048,
        log_dist=0.0,
        mel_dist=True,
        mel_scales=[2048, 1024],
        n_convs=3,
        n_linears=3,
        normalize_ola=True,
        pp_chans=5,
        pp_ker=65,
        silent_reject=[0.2, 0.2],
        spec_power=1,
        sr=22050,
        stft_scales=[2048, 1024, 512, 256],
        stride=4,
        z_dim=128,
    ):
        return stage1(
            classes=classes,
            data_dir=data_dir,
            name=name,
            continue_train=continue_train,
            batch_size=batch_size,
            learning_rate=learning_rate,
            max_steps=max_steps,
            num_workers=num_workers,
            gpus=gpus,
            precision=precision,
            profiler=profiler,
            out_dir=out_dir,
            tar_beta=tar_beta,
            beta_steps=beta_steps,
            tar_l=tar_l,
            amplitude_norm=amplitude_norm,
            channels=channels,
            env_dist=env_dist,
            h_dim=h_dim,
            kernel_size=kernel_size,
            l_grain=l_grain,
            log_dist=log_dist,
            mel_dist=mel_dist,
            mel_scales=mel_scales,
            n_convs=n_convs,
            n_linears=n_linears,
            normalize_ola=normalize_ola,
            pp_chans=pp_chans,
            pp_ker=pp_ker,
            silent_reject=silent_reject,
            spec_power=spec_power,
            sr=sr,
            stft_scales=stft_scales,
            stride=stride,
            z_dim=z_dim,
        )

    def train_latent(
        self,
        data_dir: str,
        name=None,
        waveform_name=None,
        batch_size=32,
        learning_rate=0.0002,
        max_steps=500000,
        tar_beta=0.01,
        beta_steps=500,
        conditional=False,
        num_workers=2,
        gpus=1,
        precision=32,
        profiler=False,
        out_dir="modelzoo/",
        # latent model config
        e_dim=256,
        h_dim=512,
        n_RNN=1,
        n_linears=2,
        rnn_type="LSTM",
    ):
        return stage2(
            data_dir=data_dir,
            name=name,
            waveform_name=waveform_name,
            batch_size=batch_size,
            learning_rate=learning_rate,
            max_steps=max_steps,
            tar_beta=tar_beta,
            beta_steps=beta_steps,
            conditional=conditional,
            num_workers=num_workers,
            gpus=gpus,
            precision=precision,
            profiler=profiler,
            out_dir=out_dir,
            e_dim=e_dim,
            h_dim=h_dim,
            n_RNN=n_RNN,
            n_linears=n_linears,
            rnn_type=rnn_type,
        )

    def train_hierarchical(
        self,
        data_dir: str,
        latent_name=None,
        waveform_name=None,
        batch_size=16,
        learning_rate=2e-6,  # here is the fixed learning rate at the end of the decay of the sub-network pretraining
        w_beta=0.0,
        l_beta=0.0,
        max_steps=100000,
        num_workers=2,
        gpus=1,
        precision=32,
        profiler=False,
        out_dir="modelzoo",
    ):
        return stage3(
            data_dir=data_dir,
            latent_name=latent_name,
            waveform_name=waveform_name,
            batch_size=batch_size,
            learning_rate=learning_rate,
            w_beta=w_beta,
            l_beta=l_beta,
            max_steps=max_steps,
            num_workers=num_workers,
            gpus=gpus,
            precision=precision,
            profiler=profiler,
            out_dir=out_dir,
        )

    def inference(
        self,
        latent_name,
        waveform_name,
        finetuned=False,
        model_dir="modelzoo/",
        output_dir="output/",
        samples_id=0,
        temperature=1.0,
        interp_len=4.0,
        bpm=100,
        n_bars=2,
        # pattern dict defines the generated loop:
        # each class is given a list of events [[onset1,amp1],[onset2,amp2],...]
        # onset is in [0,1] as fraction of loop_len
        # amplitude is in [0,1] ~ velocity
        pattern_dict={
            "Kick": [[0.0, 0.6], [0.2, 0.4], [0.4, 0.6], [0.6, 0.4], [0.7, 0.2], [0.8, 0.6]],
            "Snare": [[0.25, 0.5], [0.9, 0.5]],
            "Hat": [[0.1, 0.3], [0.3, 0.3], [0.5, 0.3], [0.85, 0.2]],
            "Clap": [[0.15, 0.2], [0.45, 0.2], [0.75, 0.4]],
            "Cymb_Crash_Ride": [[0.65, 0.6]],
        },
    ):
        return generate(
            latent_name=latent_name,
            waveform_name=waveform_name,
            finetuned=finetuned,
            model_dir=model_dir,
            output_dir=output_dir,
            samples_id=samples_id,
            temperature=temperature,
            interp_len=interp_len,
            bpm=bpm,
            n_bars=n_bars,
            pattern_dict=pattern_dict,
        )


if __name__ == "__main__":
    fire.Fire(Pipeline)
